{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PIXELS = 100\n",
    "imageSize = PIXELS * PIXELS\n",
    "num_features = imageSize \n",
    "\n",
    "def load_train_cv(encoder):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    print('Read train images')\n",
    "    with open('id_train.csv', 'rt') as csvfile:\n",
    "        trainreader = csv.reader(csvfile, delimiter=',')\n",
    "        next(trainreader)\n",
    "        for row in trainreader:\n",
    "            file_name = os.path.join('input', row[0] + '.jpg')\n",
    "            img = cv2.imread(file_name,0)\n",
    "            img_width = img.shape[0]\n",
    "            img_height = img.shape[1]\n",
    "            img = cv2.resize(img, (PIXELS, PIXELS))\n",
    "            feature = []\n",
    "            feature.append(img_width)\n",
    "            feature.append(img_height)\n",
    "            feature = feature + np.sum(img, 0).tolist()\n",
    "            feature = feature + np.sum(img, 1).tolist()\n",
    "            feature = feature + np.var(img, 0).tolist()\n",
    "            feature = feature + np.var(img, 1).tolist()\n",
    "            \n",
    "            X_train.append(feature)\n",
    "            y_train.append(row[1])\n",
    "            \n",
    "            M = cv2.getRotationMatrix2D((50,50),90,1)\n",
    "            \n",
    "            img90 = cv2.warpAffine(img, M, (100, 100))\n",
    "            feature = []\n",
    "            feature.append(img_height)\n",
    "            feature.append(img_width)\n",
    "            feature = feature + np.sum(img90, 0).tolist()\n",
    "            feature = feature + np.sum(img90, 1).tolist()\n",
    "            feature = feature + np.var(img90, 0).tolist()\n",
    "            feature = feature + np.var(img90, 1).tolist()\n",
    "\n",
    "            X_train.append(feature)\n",
    "            \n",
    "            if row[1] == '1':\n",
    "                y_train.append ('2')\n",
    "            elif row[1] == '2':\n",
    "                y_train.append ('1')\n",
    "            else:\n",
    "                y_train.append(row[1])\n",
    "            img180 = cv2.warpAffine(img90, M, (100, 100))\n",
    "            \n",
    "            feature = []\n",
    "            feature.append(img_height)\n",
    "            feature.append(img_width)\n",
    "            feature = feature + np.sum(img180, 0).tolist()\n",
    "            feature = feature + np.sum(img180, 1).tolist()\n",
    "            feature = feature + np.var(img180, 0).tolist()\n",
    "            feature = feature + np.var(img180, 1).tolist()\n",
    "\n",
    "            X_train.append(feature)\n",
    "            \n",
    "            y_train.append(row[1])\n",
    "            \n",
    "            img270 = cv2.warpAffine(img180, M, (100, 100))\n",
    "            \n",
    "            feature = []\n",
    "            feature.append(img_height)\n",
    "            feature.append(img_width)\n",
    "            feature = feature + np.sum(img270, 0).tolist()\n",
    "            feature = feature + np.sum(img270, 1).tolist()\n",
    "            feature = feature + np.var(img270, 0).tolist()\n",
    "            feature = feature + np.var(img270, 1).tolist()\n",
    "\n",
    "            X_train.append(feature)\n",
    "            \n",
    "            if row[1] == '1':\n",
    "                y_train.append ('2')\n",
    "            elif row[1] == '2':\n",
    "                y_train.append ('1')\n",
    "            else:\n",
    "                y_train.append(row[1])\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)#.astype('int32')\n",
    "\n",
    "    y_train = encoder.fit_transform(y_train).astype('int32')\n",
    "\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "    #X_train = X_train.reshape(X_train.shape[0], 1, PIXELS, PIXELS).astype('float32') / 255.\n",
    "    #X_test = X_test.reshape(X_test.shape[0], 1, PIXELS, PIXELS).astype('float32') / 255.\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read test images\n",
      "('Test shape:', (13999L, 402L), 'Test ID shape:', (13999L,))\n"
     ]
    }
   ],
   "source": [
    "def load_test():\n",
    "    print('Read test images')\n",
    "    X_test = []\n",
    "    X_test_id = []\n",
    "    with open('sample_submission4.csv', 'rt') as csvfile:\n",
    "        testreader = csv.reader(csvfile, delimiter=',')\n",
    "        next(testreader)\n",
    "        for row in testreader:\n",
    "            file_name = os.path.join('input', row[0] + '.jpg')\n",
    "            img = cv2.imread(file_name,0)\n",
    "            img_width = img.shape[0]\n",
    "            img_height = img.shape[1]\n",
    "            img = cv2.resize(img, (PIXELS, PIXELS))\n",
    "            feature = []\n",
    "            feature.append(img_width)\n",
    "            feature.append(img_height)\n",
    "            feature = feature + np.sum(img, 0).tolist()\n",
    "            feature = feature + np.sum(img, 1).tolist()\n",
    "            feature = feature + np.var(img, 0).tolist()\n",
    "            feature = feature + np.var(img, 1).tolist()\n",
    "            \n",
    "            X_test.append(feature)\n",
    "            X_test_id.append(row[0])\n",
    "\n",
    "    X_test = np.array(X_test)\n",
    "    X_test_id = np.array(X_test_id)\n",
    "    return X_test, X_test_id\n",
    "\n",
    "# load data\n",
    "X_test, X_test_id = load_test()\n",
    "print('Test shape:', X_test.shape, 'Test ID shape:', X_test_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 3, 2, 1, 1, 3, 0, 0, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read train images\n",
      "('Train shape:', (25600L, 402L), 'Dev (valid) shape:', (6400L, 402L))\n",
      "('Train shape:', (25600L,), 'Dev (valid) shape:', (6400L,))\n",
      "Read test images\n",
      "('Test shape:', (13999L, 100L, 100L), 'Test ID shape:', (13999L,))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "load training data and test data \n",
    "'''\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# load the training and validation data sets\n",
    "train_X, train_y, valid_X, valid_y, encoder = load_train_cv(encoder)\n",
    "print('Train shape:', train_X.shape, 'Dev (valid) shape:', valid_X.shape)\n",
    "print('Train shape:', train_y.shape, 'Dev (valid) shape:', valid_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59703125\n"
     ]
    }
   ],
   "source": [
    "#Train RF\n",
    "RF = RandomForestClassifier(n_estimators = 100)\n",
    "RF.fit(train_X, train_y)\n",
    "print(RF.score(valid_X, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61453125\n"
     ]
    }
   ],
   "source": [
    "#Train GBM with initial settings\n",
    "gbm0 = GradientBoostingClassifier(random_state=10)\n",
    "gbm0.fit(train_X, train_y)\n",
    "print(gbm0.score(valid_X, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.59297, std: 0.00877, params: {'n_estimators': 20},\n",
       "  mean: 0.61102, std: 0.00645, params: {'n_estimators': 30},\n",
       "  mean: 0.62379, std: 0.00645, params: {'n_estimators': 40},\n",
       "  mean: 0.63309, std: 0.00663, params: {'n_estimators': 50},\n",
       "  mean: 0.63957, std: 0.00503, params: {'n_estimators': 60},\n",
       "  mean: 0.64457, std: 0.00422, params: {'n_estimators': 70},\n",
       "  mean: 0.64828, std: 0.00264, params: {'n_estimators': 80}],\n",
       " {'n_estimators': 80},\n",
       " 0.648281157574221)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's tune for n_estimators\n",
    "param_test1 = {'n_estimators':range(20,81,10)}\n",
    "gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, \\\n",
    "                                                               min_samples_split=500,\\\n",
    "                                                               min_samples_leaf=50,\\\n",
    "                                                               max_depth=8,max_features='sqrt',\\\n",
    "                                                               subsample=0.8,\\\n",
    "                                                               random_state=10), \n",
    "param_grid = param_test1,n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(train_X,train_y)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.63316, std: 0.00305, params: {'min_samples_split': 200, 'max_depth': 5},\n",
       "  mean: 0.63027, std: 0.00252, params: {'min_samples_split': 400, 'max_depth': 5},\n",
       "  mean: 0.63133, std: 0.00345, params: {'min_samples_split': 600, 'max_depth': 5},\n",
       "  mean: 0.62547, std: 0.00339, params: {'min_samples_split': 800, 'max_depth': 5},\n",
       "  mean: 0.62406, std: 0.00384, params: {'min_samples_split': 1000, 'max_depth': 5},\n",
       "  mean: 0.64938, std: 0.00493, params: {'min_samples_split': 200, 'max_depth': 7},\n",
       "  mean: 0.64906, std: 0.00490, params: {'min_samples_split': 400, 'max_depth': 7},\n",
       "  mean: 0.64207, std: 0.00446, params: {'min_samples_split': 600, 'max_depth': 7},\n",
       "  mean: 0.63941, std: 0.00401, params: {'min_samples_split': 800, 'max_depth': 7},\n",
       "  mean: 0.63731, std: 0.00397, params: {'min_samples_split': 1000, 'max_depth': 7},\n",
       "  mean: 0.65832, std: 0.00489, params: {'min_samples_split': 200, 'max_depth': 9},\n",
       "  mean: 0.65344, std: 0.00592, params: {'min_samples_split': 400, 'max_depth': 9},\n",
       "  mean: 0.65102, std: 0.00572, params: {'min_samples_split': 600, 'max_depth': 9},\n",
       "  mean: 0.64859, std: 0.00301, params: {'min_samples_split': 800, 'max_depth': 9},\n",
       "  mean: 0.64516, std: 0.00385, params: {'min_samples_split': 1000, 'max_depth': 9},\n",
       "  mean: 0.65824, std: 0.00616, params: {'min_samples_split': 200, 'max_depth': 11},\n",
       "  mean: 0.65918, std: 0.00132, params: {'min_samples_split': 400, 'max_depth': 11},\n",
       "  mean: 0.65523, std: 0.00363, params: {'min_samples_split': 600, 'max_depth': 11},\n",
       "  mean: 0.65258, std: 0.00353, params: {'min_samples_split': 800, 'max_depth': 11},\n",
       "  mean: 0.64625, std: 0.00419, params: {'min_samples_split': 1000, 'max_depth': 11},\n",
       "  mean: 0.66281, std: 0.00453, params: {'min_samples_split': 200, 'max_depth': 13},\n",
       "  mean: 0.65898, std: 0.00340, params: {'min_samples_split': 400, 'max_depth': 13},\n",
       "  mean: 0.65320, std: 0.00379, params: {'min_samples_split': 600, 'max_depth': 13},\n",
       "  mean: 0.65234, std: 0.00254, params: {'min_samples_split': 800, 'max_depth': 13},\n",
       "  mean: 0.65020, std: 0.00309, params: {'min_samples_split': 1000, 'max_depth': 13},\n",
       "  mean: 0.66191, std: 0.00353, params: {'min_samples_split': 200, 'max_depth': 15},\n",
       "  mean: 0.66039, std: 0.00311, params: {'min_samples_split': 400, 'max_depth': 15},\n",
       "  mean: 0.65734, std: 0.00413, params: {'min_samples_split': 600, 'max_depth': 15},\n",
       "  mean: 0.65449, std: 0.00427, params: {'min_samples_split': 800, 'max_depth': 15},\n",
       "  mean: 0.64723, std: 0.00770, params: {'min_samples_split': 1000, 'max_depth': 15}],\n",
       " {'max_depth': 13, 'min_samples_split': 200},\n",
       " 0.66281249985995649)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick n_estimators=80\n",
    "#Lets tune for max_depth and min_samples_split\n",
    "param_test2 = {'max_depth':range(5,16,2), 'min_samples_split':range(200,1001,200)}\n",
    "gsearch2 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=80, \\\n",
    "                                                               min_samples_leaf=50,\\\n",
    "                                                               max_features='sqrt', subsample=0.8, random_state=10), \n",
    "param_grid = param_test2,n_jobs=4,iid=False, cv=5)\n",
    "gsearch2.fit(train_X,train_y)\n",
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.62805, std: 0.00365, params: {'max_features': 2, 'min_samples_leaf': 30},\n",
       "  mean: 0.62422, std: 0.00196, params: {'max_features': 2, 'min_samples_leaf': 40},\n",
       "  mean: 0.62621, std: 0.00381, params: {'max_features': 2, 'min_samples_leaf': 50},\n",
       "  mean: 0.62559, std: 0.00505, params: {'max_features': 2, 'min_samples_leaf': 60},\n",
       "  mean: 0.62559, std: 0.00632, params: {'max_features': 2, 'min_samples_leaf': 70},\n",
       "  mean: 0.64090, std: 0.00369, params: {'max_features': 4, 'min_samples_leaf': 30},\n",
       "  mean: 0.64176, std: 0.00361, params: {'max_features': 4, 'min_samples_leaf': 40},\n",
       "  mean: 0.64125, std: 0.00422, params: {'max_features': 4, 'min_samples_leaf': 50},\n",
       "  mean: 0.64344, std: 0.00454, params: {'max_features': 4, 'min_samples_leaf': 60},\n",
       "  mean: 0.64055, std: 0.00429, params: {'max_features': 4, 'min_samples_leaf': 70},\n",
       "  mean: 0.64617, std: 0.00709, params: {'max_features': 6, 'min_samples_leaf': 30},\n",
       "  mean: 0.64883, std: 0.00438, params: {'max_features': 6, 'min_samples_leaf': 40},\n",
       "  mean: 0.64805, std: 0.00398, params: {'max_features': 6, 'min_samples_leaf': 50},\n",
       "  mean: 0.64699, std: 0.00607, params: {'max_features': 6, 'min_samples_leaf': 60},\n",
       "  mean: 0.64680, std: 0.00532, params: {'max_features': 6, 'min_samples_leaf': 70},\n",
       "  mean: 0.64953, std: 0.00478, params: {'max_features': 8, 'min_samples_leaf': 30},\n",
       "  mean: 0.65238, std: 0.00467, params: {'max_features': 8, 'min_samples_leaf': 40},\n",
       "  mean: 0.64875, std: 0.00300, params: {'max_features': 8, 'min_samples_leaf': 50},\n",
       "  mean: 0.65160, std: 0.00606, params: {'max_features': 8, 'min_samples_leaf': 60},\n",
       "  mean: 0.64887, std: 0.00639, params: {'max_features': 8, 'min_samples_leaf': 70}],\n",
       " {'max_features': 8, 'min_samples_leaf': 40},\n",
       " 0.65238250671336717)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's go with 'min_samples_split' = 200 and max_depth  =13. \n",
    "#Also, let's train min_samples leaf and max_features\n",
    "param_test3 = { 'min_samples_leaf':range(30,71,10),'max_features':range(2,10,2)}\n",
    "gsearch3 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1,max_depth=13, \\\n",
    "                                                               n_estimators=80,min_samples_split=200,\\\n",
    "                                                               subsample=0.8, random_state=10), \n",
    "param_grid = param_test3,n_jobs=4,iid=False, cv=5)\n",
    "gsearch3.fit(train_X,train_y)\n",
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6603125\n"
     ]
    }
   ],
   "source": [
    "#Let's go with subsample=0.8 for now.\n",
    "#First tuned  model\n",
    "\n",
    "gbm_tuned_1 = GradientBoostingClassifier(learning_rate=0.1,max_depth=13, \\\n",
    "                                        n_estimators=80,min_samples_split=200,\\\n",
    "                                         subsample=0.8, random_state=10, max_features=8, min_samples_leaf=40)\n",
    "gbm_tuned_1.fit(train_X, train_y)\n",
    "print(gbm_tuned_1.score(valid_X, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66515625\n"
     ]
    }
   ],
   "source": [
    "#Let's tune n_estimators and learning_rate simultaneosuly. decrease by 2 and increase by 2\n",
    "gbm_tuned_2 = GradientBoostingClassifier(learning_rate=0.05,max_depth=13, \\\n",
    "                                        n_estimators=160,min_samples_split=200,\\\n",
    "                                         subsample=0.8, random_state=10, max_features=8, min_samples_leaf=40)\n",
    "gbm_tuned_2.fit(train_X, train_y)\n",
    "print(gbm_tuned_2.score(valid_X, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions\n",
      "Creating Submission\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Create Submissions - Best model \n",
    "'''\n",
    "\n",
    "import csv\n",
    "\n",
    "#make predictions\n",
    "print('Making predictions')\n",
    "preds = gbm_tuned_2.predict(X_test)\n",
    "\n",
    "def create_submission(predictions, test_id):\n",
    "     with open('submit_gbm.csv', 'wb') as mycsvfile:\n",
    "            thedatawriter = csv.writer(mycsvfile)\n",
    "            thedatawriter.writerow(['Id','label'])\n",
    "            for pred,testid  in zip(predictions,test_id):\n",
    "                out = [testid,pred+1]\n",
    "                thedatawriter.writerow(out)\n",
    "    \n",
    "\n",
    "print('Creating Submission')\n",
    "create_submission(preds, X_test_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
